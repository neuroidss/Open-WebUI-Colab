{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWyS2Y3gEyBh8NidpgxUro",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuroidss/Open-WebUI-Colab/blob/main/similar_articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KduzL391PJV",
        "outputId": "9c9e9400-ccba-49ee-ca89-79012375cb1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.10)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/Cornell-University/arxiv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1Ffn_4C1dlG",
        "outputId": "abf17f9d-01db-4d28-afec-a171e8cbed68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: neuroidss\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/Cornell-University/arxiv\n",
            "Downloading arxiv.zip to ./arxiv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.39G/1.39G [00:20<00:00, 72.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "#category_map = {\n",
        "#'cs.AI': 'Artificial Intelligence',\n",
        "#'cs.CL': 'Computation and Language',\n",
        "#'cs.CV': 'Computer Vision and Pattern Recognition',\n",
        "#'cs.IR': 'Information Retrieval',\n",
        "#'cs.LG': 'Machine Learning',\n",
        "#}\n",
        "\n",
        "#def clean_categories(x):\n",
        "#    cat_text = ''\n",
        "#    cat_list = x.split(' ')\n",
        "#    for i, item in enumerate(cat_list):\n",
        "#        cat_name = category_map[item]\n",
        "#        if cat_name != 'Not available':\n",
        "#            if i == 0: cat_text = cat_name\n",
        "#            else:      cat_text = cat_text + ', ' + cat_name\n",
        "#    cat_text = cat_text.strip()\n",
        "#    return cat_text\n",
        "\n",
        "def clean_text(x):\n",
        "    new_text = x.replace(\"\\n\", \" \")\n",
        "    new_text = new_text.strip()\n",
        "    return new_text\n",
        "\n",
        "def clean_dataframe():\n",
        "    columns = ['id', 'authors', 'title', 'categories', 'abstract']\n",
        "\n",
        "    # Specify the folders containing the JSON and schema files\n",
        "#    filepath = '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\n",
        "    filepath = 'arxiv/'+'arxiv-metadata-oai-snapshot.json'\n",
        "    data = []\n",
        "\n",
        "    # Open the file and read line by line\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            doc = json.loads(line)\n",
        "            lst = [doc['id'], doc['authors'], doc['title'], doc['categories'], doc['abstract']]\n",
        "            data.append(lst)\n",
        "\n",
        "    arxiv_data = pd.DataFrame(data=data, columns=columns)\n",
        "#    topics = ['cs.AI', 'cs.CV', 'cs.IR', 'cs.LG', 'cs.CL']\n",
        "\n",
        "#    filtered_data                  = arxiv_data[arxiv_data['categories'].isin(topics)]\n",
        "    filtered_data                  = arxiv_data\n",
        "#    filtered_data['categories']    = filtered_data['categories'].apply(clean_categories)\n",
        "    filtered_data['title']         = filtered_data['title'].apply(clean_text)\n",
        "    filtered_data['abstract']      = filtered_data['abstract'].apply(clean_text)\n",
        "    filtered_data['prepared_text'] = filtered_data['title'] + ' \\n ' + filtered_data['abstract']\n",
        "    return filtered_data\n",
        "\n",
        "df = clean_dataframe()\n",
        "df.to_csv('arxiv/'+'arxiv-metadata-oai-snapshot.csv', encoding='utf-8', index=False)\n"
      ],
      "metadata": {
        "id": "kZiyyT-O1oNF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "arxiv_data = pd.read_csv('arxiv/'+\"arxiv-metadata-oai-snapshot.csv\")\n",
        "\n",
        "#topics = ['cs.AI', 'cs.CV', 'cs.IR', 'cs.LG', 'cs.CL']\n",
        "\n",
        "#filtered_data                  = arxiv_data[arxiv_data['categories'].isin(topics)]\n",
        "#filtered_data                  = arxiv_data\n",
        "#filtered_data['prepared_text'] = filtered_data['title'] + ' \\n ' + filtered_data['abstract']\n",
        "\n",
        "#print(filtered_data.shape)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a sample DataFrame\n",
        "#df = pd.DataFrame({\n",
        "#    'Name': ['Alice', 'Bob', 'Charlie'],\n",
        "#    'Age': [25, 30, 35],\n",
        "#    'Country': ['USA', 'Canada', 'Mexico']\n",
        "#})\n",
        "\n",
        "# Define a search function\n",
        "def search_string(s, search):\n",
        "    return search in str(s).lower()\n",
        "\n",
        "filtered_arxiv_data = arxiv_data.loc[:, arxiv_data.columns == 'prepared_text']\n",
        "\n",
        "# Search for the string 'al' in all columns\n",
        "mask = filtered_arxiv_data.apply(lambda x: x.map(lambda s: search_string(s, 'eeg')))\n",
        "\n",
        "# Filter the DataFrame based on the mask\n",
        "filtered_arxiv_data = filtered_arxiv_data.loc[mask.any(axis=1)]\n",
        "\n",
        "# Search for the string 'al' in all columns\n",
        "mask = filtered_arxiv_data.apply(lambda x: x.map(lambda s: search_string(s, 'working')))\n",
        "\n",
        "# Filter the DataFrame based on the mask\n",
        "filtered_arxiv_data = filtered_arxiv_data.loc[mask.any(axis=1)]\n",
        "\n",
        "# Search for the string 'al' in all columns\n",
        "mask = filtered_arxiv_data.apply(lambda x: x.map(lambda s: search_string(s, 'memory')))\n",
        "\n",
        "# Filter the DataFrame based on the mask\n",
        "filtered_arxiv_data = filtered_arxiv_data.loc[mask.any(axis=1)]\n",
        "\n",
        "print(filtered_arxiv_data)\n",
        "\n",
        "filtered_arxiv_data.to_csv('arxiv/'+'arxiv-metadata-oai-snapshot_eeg_working_memory.csv', encoding='utf-8', index=False)\n",
        "\n",
        "import os\n",
        "\n",
        "filename = 'arxiv/'+\"arxiv-metadata-oai-snapshot_working_memory/out.csv\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "for index, row in filtered_arxiv_data.iterrows():\n",
        "    row.to_csv('arxiv/'+'arxiv-metadata-oai-snapshot_working_memory/out_{index}.csv'.format(index=index), encoding='utf-8', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-EusuEB1szk",
        "outputId": "ba3f154c-2707-4254-e9fe-812d75539165"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-eb587e88e4bd>:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  arxiv_data = pd.read_csv('arxiv/'+\"arxiv-metadata-oai-snapshot.csv\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             prepared_text\n",
            "671220   Filter Bank Common Spatial Patterns in Mental ...\n",
            "721571   Comparison of Network Analysis Approaches on E...\n",
            "756442   Assessing Functional Neural Connectivity as an...\n",
            "858570   Decline of long-range temporal correlations in...\n",
            "858571   The interplay between long- and short-range te...\n",
            "952632   Use of Eye-Tracking Technology to Investigate ...\n",
            "1189540  Decoding Working Memory Load from EEG with LST...\n",
            "1256255  Inhibition and Set-Shifting Tasks in Central E...\n",
            "1290981  EEG source localization analysis in epileptic ...\n",
            "1311364  Application of statistical analysis to working...\n",
            "1332709  Single-channel EEG features during n-back task...\n",
            "1369789  BCI learning induces core-periphery reorganiza...\n",
            "1427483  Coherence of Working Memory Study Between Deep...\n",
            "1432301  Ridge-penalized adaptive Mantel test and its a...\n",
            "1433999  A Pilot Study on Visually Stimulated Cognitive...\n",
            "1484533  Cross-Subject Domain Adaptation for Classifyin...\n",
            "1566256  Methodology and feasibility of neurofeedback t...\n",
            "1576240  Differential EEG Characteristics during Workin...\n",
            "1638716  Is the Contralateral Delay Activity (CDA) a ro...\n",
            "1667376  Infrared Radiation of Graphene Electrothermal ...\n",
            "1690368  Significant changes in EEG neural oscillations...\n",
            "1761831  Changes in Power and Information Flow in Resti...\n",
            "1951670  Impact of Nap on Performance in Different Work...\n",
            "1953414  Designing and Evaluating an Adaptive Virtual R...\n",
            "1988407  Using i-vectors for subject-independent cross-...\n",
            "2016128  FAST functional connectivity implicates P300 c...\n",
            "2056918  Bayesian Functional Connectivity and Graph Con...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openwebui_host=''\n",
        "token=\"sk-7c61dffdebe14dd992b0ecf2b9c4196f\""
      ],
      "metadata": {
        "id": "HerTkwqW5WmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def upload_file(token, file_path):\n",
        "    url = 'http://'+openwebui_host+':3000/api/v1/files/'\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {token}',\n",
        "        'Accept': 'application/json'\n",
        "    }\n",
        "    files = {'file': open(file_path, 'rb')}\n",
        "    response = requests.post(url, headers=headers, files=files)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "Mz0iN_Js4rZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def add_file_to_knowledge(token, knowledge_id, file_id):\n",
        "    url = f'http://'+openwebui_host+':3000/api/v1/knowledge/{knowledge_id}/file/add'\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {token}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    data = {'file_id': file_id}\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "jSknImJG4tvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "file_ids=[]\n",
        "for file in os.listdir('arxiv/'+'arxiv-metadata-oai-snapshot_working_memory'):\n",
        "  if file.endswith(\".csv\"):\n",
        "    print(os.path.join('arxiv/'+'arxiv-metadata-oai-snapshot_working_memory', file))\n",
        "    file_path='arxiv/'+'arxiv-metadata-oai-snapshot_working_memory/'+file\n",
        "    response_json = upload_file(token, file_path)\n",
        "    print(response_json)\n",
        "#    knowledge_id\n",
        "    file_id=response_json['id']\n",
        "    file_ids.append(file_id)\n",
        "#    add_file_to_knowledge(token, knowledge_id, file_id)\n",
        "#    break"
      ],
      "metadata": {
        "id": "_w88AZlH4btX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def chat_with_file(token, model, query, file_id):\n",
        "    url = 'http://'+openwebui_host+':3000/api/chat/completions'\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {token}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    payload = {\n",
        "        'model': model,\n",
        "        'messages': [{'role': 'user', 'content': query}],\n",
        "        'files': [{'type': 'file', 'id': file_id}]\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "def chat_with_files(token, model, query, file_ids):\n",
        "    url = 'http://'+openwebui_host+':3000/api/chat/completions'\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {token}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    files = []\n",
        "    for file_id in file_ids:\n",
        "        files.append({'type': 'file', 'id': file_id})\n",
        "    payload = {\n",
        "        'model': model,\n",
        "        'messages': [{'role': 'user', 'content': query}],\n",
        "        'files': files\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "LgyGtkuS42gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=\"deepseekr1-coder-14b\"\n",
        "query='make psytrance visuals, controlled by simulated eeg stream, based on working memory increase science, put css and js inside single html file. all comments leave only inside html block. no text output outside html block'\n",
        "for file_id in file_ids:\n",
        "#  response_json = chat_with_file(token, model, query, file_id)\n",
        "  file_ids_plus = file_ids.copy()\n",
        "  file_ids_plus.append(file_id)\n",
        "  response_json = chat_with_files(token, model, query, file_ids_plus)\n",
        "\n",
        "  print(response_json)\n",
        "  import os\n",
        "\n",
        "  filename = 'arxiv/'+\"arxiv-metadata-oai-snapshot_working_memory/html/out.html\"\n",
        "  os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "  filename = 'arxiv/'+'arxiv-metadata-oai-snapshot_working_memory/html/out_'+file_id+'.html'\n",
        "  f = open(filename, \"w\")\n",
        "  response_message_content = response_json['choices'][0]['message']['content']\n",
        "  created_html = ''\n",
        "  for response_message_content_split in response_message_content.split(\"```\"):\n",
        "    split_html = response_message_content_split.split(\"html\\n\")\n",
        "    if len(split_html)>1 and len(split_html[0])==0 and len(created_html)==0:\n",
        "      created_html = split_html[1]\n",
        "  f.write(created_html)\n",
        "  f.close()\n",
        "#  import IPython\n",
        "#  IPython.display.HTML(filename=filename)\n",
        "#  from IPython.display import IFrame\n",
        "#  IFrame(src=filename, width=900, height=600)"
      ],
      "metadata": {
        "id": "Esbn_CDh45Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def chat_with_collection(token, model, query, collection_id):\n",
        "    url = 'http://'+openwebui_host+':3000/api/chat/completions'\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {token}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    payload = {\n",
        "        'model': model,\n",
        "        'messages': [{'role': 'user', 'content': query}],\n",
        "        'files': [{'type': 'collection', 'id': collection_id}]\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "u0FHJrAw481c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}